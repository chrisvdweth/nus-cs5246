{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb88727-b69b-46c8-9b06-b5f83cc55e54",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19d0a9-13f7-408c-82f8-ae7dbef28125",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "Clustering is an unsupervised machine learning technique used to group similar data points together based on their characteristics or features. In the context of text documents, clustering algorithms organize documents into groups or clusters where documents within the same cluster are more similar to each other compared to documents in other clusters.\n",
    "\n",
    "Here's how clustering can be applied to organize text documents:\n",
    "\n",
    "* **Representation of Text Documents:** Before clustering, text documents need to be transformed into numerical representations. Techniques like TF-IDF, word embeddings (such as Word2Vec or GloVe), or document vectors using methods like Doc2Vec can convert text into feature vectors that capture semantic information.\n",
    "\n",
    "* **Choice of Clustering Algorithm:** Several clustering algorithms can be used for text document organization, such as K-Means, Hierarchical Clustering, DBSCAN, or affinity propagation. Each algorithm has its own way of defining clusters based on distance metrics, density, or connectivity.\n",
    "\n",
    "* **Clustering Process:** Once the text documents are represented as numerical vectors, the chosen clustering algorithm groups similar documents together. The algorithm identifies patterns or similarities in the feature space and iteratively assigns documents to clusters based on certain criteria.\n",
    "\n",
    "* **Evaluation and Interpretation:** Clustering algorithms often require parameters (e.g., number of clusters for K-Means) that can impact the clustering results. Evaluation metrics like silhouette score or coherence measures can help assess the quality of clusters. Additionally, interpreting the clusters by analyzing the content of documents within each cluster can provide insights into the underlying structure or themes in the document collection.\n",
    "\n",
    "* **Applications:** Clustering text documents finds applications in various areas, such as information retrieval, document organization, topic modeling, and recommendation systems. It can be used to automatically group similar articles, classify news topics, organize search results, or create document archives with related content.\n",
    "\n",
    "Clustering facilitates the organization and exploration of large text document collections by grouping similar documents together, allowing for easier management, analysis, and understanding of textual data. It aids in discovering hidden structures and patterns within text corpora, enabling better information retrieval and knowledge discovery. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767575d-c9d2-495f-88c8-5e9a0c5947d4",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bab8a-49d2-46a4-b17f-79dcb5da5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee3e4-6563-4cb6-98ae-8a507fe3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56e369-8cc7-45fb-a385-41b0a402942a",
   "metadata": {},
   "source": [
    "### Auxiliary Code\n",
    "\n",
    "The file `src/utils.py` contains a series of auxiliary methods to plot the clustering results as well as for fetching the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9797b-de62-4051-8431-298ee3bdb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_articles, get_random_article, color_func, get_mask, plot_sse_values, plot_silhouette_scores, plot_cluster_wordcloud, plot_dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e032263-426b-4d06-87c5-5188017e988c",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "The file `data/news-articles-preprocessed.zip` contains a text file with 6k+ news articles collected from The Straits Times around Oct/Nov 2022. The articles are already somewhat preprocessed (punctuation removal, converted to lowercase, line break removal, lemmatization). Each line in the text file represents an individual article.\n",
    "\n",
    "To get the article, the method `get_articles()` reads this zip file and loops through the text file and returns all articles in a list. The method also accepts a `search_term` to filter articles that contain that search term. While not used by default in the following, you can check it out to get different results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7db98-0ed7-4c9d-afb7-20ad9ba29ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip')\n",
    "#articles = get_articles(search_term=\"police\")\n",
    "\n",
    "print(\"Number of articles: {}\".format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1bbb9-e97f-4686-b527-b1e90fbdf701",
   "metadata": {},
   "source": [
    "There is also a method `get_random_article()` which, to the surprise of no-one, returns a random article from the list of 6k+ articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd695fac-4798-4951-891e-1fd239c43ad8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f60f3-70a8-4ba3-b705-d62f3d579e5c",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "The K-Means algorithm is an unsupervised machine learning technique used for clustering data into distinct groups or clusters based on similarity in their features. It aims to partition a dataset into K clusters, where each data point belongs to the cluster with the nearest mean (centroid).\n",
    "\n",
    "Here's an overview of how the K-Means algorithm works:\n",
    "\n",
    "* **Initialization:** Begin by choosing K initial cluster centroids randomly from the dataset. These centroids represent the centers of the clusters.\n",
    "\n",
    "* **Assignment Step:** For each data point in the dataset, calculate the distance (typically using Euclidean distance) between that point and each of the K centroids. Assign the data point to the cluster represented by the nearest centroid.\n",
    "\n",
    "* **Update Step:** Recalculate the centroids of the K clusters based on the newly assigned data points. The new centroid of each cluster is the mean of all data points assigned to that cluster along each feature dimension.\n",
    "\n",
    "* **Iterations:** Repeat the assignment and update steps iteratively until convergence. Convergence occurs when the assignment of data points to clusters no longer changes or when a maximum number of iterations is reached.\n",
    "\n",
    "* **Final Result:** Once the algorithm converges, the data points are clustered into K groups, and each data point belongs to the cluster represented by the nearest centroid.\n",
    "\n",
    "Key considerations and characteristics of the K-Means algorithm:\n",
    "\n",
    "* It's sensitive to the initial placement of centroids, and different initializations can lead to different final clusters.\n",
    "* K-Means aims to minimize the sum of squared distances between data points and their respective cluster centroids.\n",
    "* The number of clusters, K, is a hyperparameter that needs to be determined based on domain knowledge or using techniques like the elbow method or silhouette score.\n",
    "* It's an efficient and scalable algorithm, but it may struggle with non-linear or irregularly shaped clusters and is sensitive to outliers.\n",
    "\n",
    "K-Means clustering is widely used in various domains for tasks like customer segmentation, image segmentation, anomaly detection, and more, where grouping similar data points together is essential for analysis and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba75596-9cc8-46b7-ba71-619b176ae113",
   "metadata": {},
   "source": [
    "### Varying the Number of Clusters\n",
    "\n",
    "Let's first have a look how the choice of *k* (i.e., the number of clusters) affect the result. Since K-Means on large data may take some time, we limit our articles to ones that only contain the word *\"police\"*. Of course, you can try out different keywords or simply consider all articles by omitting the `search_term` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cafb07-c64c-44ef-8ce9-49b760ec500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip', search_term=\"police\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243027a-6d99-4d3d-978b-233aceb8b41f",
   "metadata": {},
   "source": [
    "As usual, we need to convert our articles into document vectors; we use the TF-IDF weights. Note that we limit the number of considered terms (i.e., the vocabulary size) to 2,000. Again, the reason is mainly to speed up the computation time of all K-Means clusters. Given that we only consider a couple of hundred articles, 2,000 is actually not that low to be considered meaningful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25cb96-578a-41fc-a59c-252fc64ef457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 1), max_features=2000)\n",
    "\n",
    "# Transform documents to tf-idf vectors (Term-Document Matrix with tf-idf weights)\n",
    "tfidf = tfidf_vectorizer.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64d4dc-1432-436c-9fa5-9c921c8adf8e",
   "metadata": {},
   "source": [
    "Now we're ready to perform K-Means clustering. As the goal is to see how the result differ for different *k* we simple execute a loop and compute in each iteration:\n",
    "\n",
    "* K-Means for the current value of *k*\n",
    "* The SSE value for the current *k*\n",
    "* The Silhouette Coefficient for the current *k*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a87385-c259-4194-a28f-3bea03559c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_k = 200\n",
    "\n",
    "sse, silhouette_scores = [], []\n",
    "\n",
    "for k in tqdm(range(2, num_clusters_k+1)):\n",
    "    kmeans_toy = KMeans(n_clusters=k, n_init='auto', random_state=0).fit(tfidf.A)\n",
    "    \n",
    "    cluster_labels = kmeans_toy.fit_predict(tfidf.A)    \n",
    "    silhouette_avg = silhouette_score(tfidf.A, cluster_labels)\n",
    "    \n",
    "    sse.append((k, kmeans_toy.inertia_)) # Inertia is the same as SSE\n",
    "    silhouette_scores.append((k, silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535ad43-418a-4eb4-9f76-da2d7ae3bf5f",
   "metadata": {},
   "source": [
    "The method `plot_sse_values()` plots the SSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb71f9-f182-4fd1-a38a-8660810853f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sse_values(sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7982f-80d6-4757-ae27-ea806945b36e",
   "metadata": {},
   "source": [
    "As we already know, increasing *k* will always lower the SSE value (until it converges to its minimum -- which depends on *k* and the data distribution). More importantly, however, there is no visible \"elbow\" that would tell us about the best value for *k*.\n",
    "\n",
    "Similar to the SSE values, we can also plot the Silhouette Coefficients for all considered values of *k*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffe739-eef0-4328-9dd9-410d3c7c23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_scores(silhouette_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004eb864-8f9c-4f7a-852b-9c5b80db1213",
   "metadata": {},
   "source": [
    "The results are quite similar. In a nutshell, there is no obvious indicator what constitutes the best value for *k*. As we discussed in the lecture and in the supplementary recording, the issue is that our document vectors have many features, i.e., our feature space is high-dimensional. Simply speaking, our data distribution is arbitrarily unlikely to feature nice, well-separated \"blobs\" which would yield more useful SSE values and Silhouette Coefficients. This is why in practice the choice of *k* are commonly a very pragmatic decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42942b-b796-4cd9-a3be-d132bad26c32",
   "metadata": {},
   "source": [
    "### Inspecting Clusters\n",
    "\n",
    "In practice, clustering is often used to get a basic understanding of the dataset, e.g., a text corpus. This can be done by clustering the corpus and inspecting each cluster, e.g., by visualizing a cluster w.r.t. to its most important words. So let's do this using a basic approach. Since we run K-Means only once in the following, we can be a bit more generous regarding the number of news articles and the number of features. So feel free to play with those parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f2aa9-f4e0-41a9-b86f-f572b7167dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip')\n",
    "#articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip', search_term=\"police\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b980a3-0d84-4847-b15f-bee49014cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=5, smooth_idf=False, max_features=5000)\n",
    "\n",
    "# Transform documents to tf-idf vectors (Term-Document Matrix with tf-idf weights)\n",
    "tfidf = tfidf_vectorizer.fit_transform(articles)\n",
    "\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69494e48-5bbd-4035-8f66-d67bd010d393",
   "metadata": {},
   "source": [
    "Let's run K-Mean with $k=5$ to get 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4ccd5-1c31-4407-b79e-8d72eb4d122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_clusters_k = 5\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters_k, n_init='auto', random_state=0).fit(tfidf.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82d544-f24e-4b24-ab3e-ad3c95464c6e",
   "metadata": {},
   "source": [
    "To visualize the resulting clusters, we provide you with the method `plot_cluster_wordcloud()` that creates a word cloud using the most important words in the respective cluster, where the importance of a word directly derives from the TF-IDF weight. You're encouraged to have a look at the method as it requires some understanding of what's going on. But here, we are just interested in the word clouds.\n",
    "\n",
    "The code cell below goes through all cluster IDs from $0..(k-1)$ and plots the word cloud for that cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12b407-6ec9-46c0-b6fb-0ef9d7345a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in range(num_clusters_k):\n",
    "    plot_cluster_wordcloud(tfidf_vectorizer, kmeans, cid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe9bc3-18c5-4134-a458-eaefe11e7524",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3aee6-9983-40ec-be34-942bb87dd729",
   "metadata": {},
   "source": [
    "## AGNES\n",
    "\n",
    "In contrast to K-Means, AGNES (Agglomerative Nesting) is one of the most popular hierarchical clustering algorithms. Agglomerative Hierarchical Clustering is an approach that starts by considering each data point as a single cluster and then merges pairs of clusters iteratively to form a hierarchy of clusters. It proceeds by following these steps:\n",
    "\n",
    "* **Initialization:** Treat each data point as an individual cluster. Assign each point to its own cluster, making as many clusters as there are data points.\n",
    "\n",
    "* **Similarity/Dissimilarity Calculation:** Compute the distance or dissimilarity between each pair of clusters. Various distance metrics (such as Euclidean distance, Manhattan distance, or other similarity measures) can be used to determine the distance between clusters.\n",
    "\n",
    "* **Merging Clusters:** At each iteration, the two most similar clusters are merged together based on a linkage criterion. Common linkage methods include:\n",
    "\n",
    "    * Single Linkage: Merge clusters based on the minimum distance between points in the two clusters.\n",
    "    * Complete Linkage: Merge clusters based on the maximum distance between points in the two clusters.\n",
    "    * Average Linkage: Merge clusters based on the average distance between points in the two clusters.\n",
    "    * Ward's Method: Minimizes the variance when merging clusters.\n",
    "\n",
    "* **Hierarchical Structure Building:** Repeat the process of merging the most similar clusters until all data points belong to a single cluster or until a stopping criterion (e.g., a specific number of clusters or a threshold distance) is met.\n",
    "\n",
    "* **Dendrogram Formation:** As the clusters are merged, a dendrogram—a tree-like structure—is created, illustrating the sequence of merges and the distance at which they occurred. The dendrogram helps visualize the hierarchical relationship between clusters.\n",
    "\n",
    "Agglomerative Hierarchical Clustering results in a hierarchical decomposition of the dataset, where the clusters can be observed at different levels of granularity. This method does not require specifying the number of clusters beforehand, making it useful for exploring the structure of the data and identifying natural groupings. It's important to note that the complexity of Agglomerative Hierarchical Clustering can be computationally expensive for large datasets due to its iterative nature and the need to calculate pairwise distances between data points.\n",
    "\n",
    "\n",
    "\n",
    "### Visualizing the Complete Hierarchy\n",
    "\n",
    "In hierarchical clustering, a dendrogram is a tree-like diagram that displays the arrangement of clusters and their relationships at various stages of the clustering process. It visually represents the merging of clusters and helps illustrate the hierarchy of relationships between data points or clusters.\n",
    "\n",
    "Key elements of a dendrogram:\n",
    "\n",
    "* **Vertical Axis:** The vertical axis of the dendrogram represents the dissimilarity or distance between clusters or data points. The height or level at which two clusters merge or a data point joins a cluster indicates the distance at which the merge occurred.\n",
    "\n",
    "* **Horizontal Axis:** The horizontal axis shows the individual data points or clusters. Each point on this axis represents a data point initially, and as clusters merge, they form branches on the dendrogram.\n",
    "\n",
    "* **Branches and Merges:** The branches of the dendrogram represent the clusters formed at different stages of the clustering process. The height of each branch's fusion indicates the dissimilarity level at which the clusters were merged.\n",
    "\n",
    "* **Interpretation:** The structure of the dendrogram allows for interpretation of the relationships between clusters or data points. By observing the vertical lines where clusters merge, one can determine the distance or dissimilarity measure at which these merges occurred, providing insights into the similarity or distance between clusters.\n",
    "\n",
    "* **Cluster Cutting:** Dendrograms allow for cutting the tree at different heights or distances, resulting in different numbers of clusters. This cutting process enables the determination of the optimal number of clusters based on the problem's requirements or characteristics of the data.\n",
    "\n",
    "Dendrograms are valuable visualization tools in hierarchical clustering, aiding in understanding the structure and relationships within the dataset. They provide a clear representation of how clusters merge and form a hierarchy, allowing for the identification of clusters at different levels of granularity based on the distance or similarity thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d2959-aad5-4ccf-9e83-99e01e5a4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip', search_term=\"tesla\")\n",
    "\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fff25-815b-40ed-8a90-149f41c52747",
   "metadata": {},
   "source": [
    "Again, feel free to play around with the parameters for converting the news articles into document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e67f9-c1f0-48c3-8efc-c999182fe521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 1), max_features=2000)\n",
    "\n",
    "# Transform documents to tf-idf vectors (Term-Document Matrix with tf-idf weights)\n",
    "tfidf = tfidf_vectorizer.fit_transform(articles)\n",
    "\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018b171-a1f7-4ac0-ba20-35acbbc49baa",
   "metadata": {},
   "source": [
    "Our document vectors are all we need to run AGNES. As it's common with scikit-learn algorithms, this just boils down to calling the `fit()` method. However, we need to make it explicit that indeed the complete hierarchy should be calculated. Adopting the code from this [scikit-learn page](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html), method `plot_dendrogram()` will plot the dendrogram for a given AGNES clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e2731-6612-4a6b-b22f-7693c3bc07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "agnes = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(tfidf.A)\n",
    "\n",
    "# As labels for the x-axis we use the first 20 characters of the article;\n",
    "# admittely it's not very useful, but it's quick and easy\n",
    "xtics = [ d[:20] for d in articles]\n",
    "\n",
    "# Plot the dendrogram\n",
    "plot_dendrogram(agnes, xtics, truncate_mode=\"level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be263daa-8c68-4883-9a5e-0872fcd33792",
   "metadata": {},
   "source": [
    "Recall that the height of the merges (i.e., the horizontal bars) reflect the distances between the two merged clusters. If you check the dendrogram above, you can see that some individual articles got merged with a distance of 0. This naturally indicates that our corpus of news articles contains a couple of duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecadaa3-1b25-471e-b98e-d06bcebd0d65",
   "metadata": {},
   "source": [
    "### Get *k* Clusters with AGNES\n",
    "\n",
    "In practice, we can use AGNES similar to K-Means by specifying a fixed number of *k* clusters. This means that AGNES can stop with merging cluster the moment *k* clusters have been formed. The basic output is (again, like with K-Means) a list containing the cluster ID for each of the input documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f8600-3943-454b-a51f-67c5b00a457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes = AgglomerativeClustering(n_clusters=5).fit(tfidf.A)\n",
    "\n",
    "print(agnes.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2120d-63ed-49be-867b-d2d529d109e8",
   "metadata": {},
   "source": [
    "In the output above, the first entry represents the first news article, the second entry represents the second news article, and so on. If the values of 2 articles are identical, this means that these 2 articles belong to the same cluster. A natural step would be, again, to maybe visualize each cluster. However, we cannot use `plot_cluster_wordcloud()` here, since this method is specific to K-Means as it utilizes the final centroids for the visualization. To come up with an alternative visualization is up to you :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec525fc-f5f7-4943-ae2c-f5a92af0b7a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a203b49-dd87-4a8a-bc11-c7190e28b60a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Clustering is a very common data mining / text mining technique to either organize data based on their similarities / distance. This can be used as a solution for a task (i.e., organize all text documents into 20 groups) or as some form of Exploratory Data Analysis (EDA) to get a basic understanding for the given data. Clustering is considered a generic technique as it can always be applied once a meaningful notion of similarity / distance is defined between two data points (e.g., text documents). There are no other requirements.\n",
    "\n",
    "Clustering methods like K-Means and AGNES (Agglomerative Hierarchical Clustering) are valuable in organizing text documents by grouping them based on similarities in content. Here's a summary of their uses:\n",
    "\n",
    "* **K-Means Clustering for Text Documents:** K-Means is effective for organizing text documents by partitioning them into K distinct clusters. It helps in:\n",
    "    * Grouping Similar Documents: K-Means identifies clusters of documents sharing similar content, aiding in document organization and retrieval.\n",
    "    * Topic Discovery: By categorizing documents into clusters, K-Means can reveal underlying topics or themes within the text corpus.\n",
    "    * Document Summarization: It enables summarization by selecting representative documents from each cluster, condensing large volumes of text into manageable summaries.\n",
    "\n",
    "* **AGNES (Agglomerative Hierarchical Clustering) for Text Documents:** AGNES constructs a hierarchy of clusters by iteratively merging similar clusters. Its uses for text documents include:\n",
    "    * Hierarchical Structure: AGNES forms a hierarchical representation of clusters, providing insights into relationships between documents at different levels of granularity.\n",
    "    * Dendrogram Visualization: The dendrogram produced by AGNES helps visualize the merging process and offers an intuitive view of document relationships and hierarchy.\n",
    "    * Flexible Clustering: AGNES allows the determination of clusters at various levels of similarity, offering flexibility in organizing documents based on different levels of detail or abstraction.\n",
    "\n",
    "In essence, K-Means and AGNES are powerful tools for organizing text documents by clustering them into groups that share common content or themes. K-Means partitions documents into distinct clusters, aiding in summarization and topic discovery, while AGNES constructs hierarchical structures, offering insights into document relationships at different levels of granularity. Both methods contribute significantly to document management, analysis, and understanding within text corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222dace3-053a-4ff9-9a52-ec9ecbb12ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
