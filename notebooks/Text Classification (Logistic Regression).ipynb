{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ab2016-9be3-4e18-8582-2c46d3b87b8a",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19d0a9-13f7-408c-82f8-ae7dbef28125",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method used for binary classification tasks, predicting the probability of a binary outcome based on input features. Despite its name, it's used for classification, not regression. Here's how Logistic Regression works and its application in text document classification:\n",
    "\n",
    "* **Algorithm Overview:** Logistic Regression models the relationship between a set of independent variables (features) and a binary dependent variable (the target class). It uses the logistic function (also called the sigmoid function) to map input features to a probability score between 0 and 1, representing the likelihood of belonging to a particular class.\n",
    "\n",
    "* **Text Document Classification:** In the context of text classification, each document is represented as a numerical feature vector. This vector can be constructed using various methods, such as:\n",
    "    * *Bag-of-Words (BoW):* Counts the frequency of words in each document, creating a numerical representation.\n",
    "    * *TF-IDF (Term Frequency-Inverse Document Frequency):* Weights words based on their frequency in a document relative to their frequency across all documents, capturing word importance.\n",
    "    * *Word Embeddings:* Dense, lower-dimensional representations of words that capture semantic relationships and context.\n",
    "\n",
    "\n",
    "* **Model Training:** Once text documents are transformed into feature vectors, Logistic Regression fits a linear decision boundary to distinguish between classes based on these features. It learns the weights (coefficients) for each feature and the bias term to make predictions.\n",
    "\n",
    "* **Prediction:** During prediction, Logistic Regression uses these learned weights and the logistic (sigmoid) function to calculate the probability of a document belonging to a particular class. A threshold (usually 0.5) is applied to these probabilities to classify documents into the respective classes.\n",
    "\n",
    "* **Regularization:** Logistic Regression models can be regularized to prevent overfitting by adding penalty terms to the objective function. Regularization techniques like L1 or L2 regularization help control model complexity and improve generalization to new data.\n",
    "\n",
    "Logistic Regression is often considered a baseline model for text classification tasks due to its simplicity, interpretability, and efficiency. It's particularly useful when the relationship between features and classes is relatively linear or when there's a need for a straightforward probabilistic interpretation of the results.\n",
    "\n",
    "Overall, Logistic Regression is a versatile and widely used method in text document classification, providing a probabilistic framework to predict class probabilities based on textual features, aiding in tasks like sentiment analysis, document categorization, or spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767575d-c9d2-495f-88c8-5e9a0c5947d4",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bab8a-49d2-46a4-b17f-79dcb5da5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee3e4-6563-4cb6-98ae-8a507fe3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91ae81-26ca-42dd-823d-abff242623a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95605cb-d78c-4176-aff3-89f787d4c559",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "For this notebook, we use a simple dataset for sentiment classification. This dataset consists of 10,662 sentences, where 50% of the sentences are labeled 1 (positive), and 50% of the sentences are labeled -1 (negative).\n",
    "\n",
    "### Loading Sentence/Label Pairs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4014a-4e71-4705-b448-35ed3ffb22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/sentence-polarities.csv\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        sentence, label = line.split(\"\\t\")\n",
    "        sentences.append(sentence)\n",
    "        labels.append(int((int(label)+1)/2))\n",
    "        \n",
    "print(\"Total number of sentences: {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c8fc-19f5-46b8-b4ae-c5020a54c0d4",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8e763-c7f7-44db-ab32-e799eab2ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences and labels into training and test set with a test set size of 20%\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# We can directly convert the numerical class labels from lists to numpy arrays\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208ab65-24fc-4b86-8054-55b91c7dfac4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec73c2-0567-46da-9f04-9393e8a933ce",
   "metadata": {},
   "source": [
    "## Training & Testing a Logistic Classifier\n",
    "\n",
    "Let's first have a look at how to train a Naive Bayes classifier with the minimum number of steps. For this, we randomly pick some meaningful values for the vectorizer and use the the default values of the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cb848-5324-4f6c-bb57-8fa12f965b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix for differen n-gram sizes\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "X_test = tfidf_vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef9346-5773-4d17-9bbc-474cd60df1f4",
   "metadata": {},
   "source": [
    "Using the training data, we can train a Naive Bayes classifier with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd90a07-2ef2-4085-a070-4a62f2c57636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db8bf4-dbdb-41cd-9851-8c073a845e1f",
   "metadata": {},
   "source": [
    "Once trained, we can predict the class labels for the document vectors in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c60195-54ee-4982-82dc-ffe902a9b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f6439-b444-4477-921b-47e8aaf6acef",
   "metadata": {},
   "source": [
    "`y_pred` now contains the 2,133 predicted labels that we can compare with the ground truth labels from the test set. scikit-learn provides methods to easily calculate all the important metrics we covered in the lecture. Since we only have to class labels (i.e., binary classification), we do not have to set the `average` parameter to indicate micro or macro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03669c7f-8804-439e-bdc2-bbb92029db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precison: {:.3f}\".format(precision))\n",
    "print(\"Recall:   {:.3f}\".format(recall))\n",
    "print(\"F1 score: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13cf18-70b3-4df1-8172-c27c79c740a8",
   "metadata": {},
   "source": [
    "scikit-learn also provides a method `classification_report()` for a more detailed description of the results, showing a breakdown of the precision, recall, and f1 scores broken down for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eae60-292b-43ed-879f-f10e6f063126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070f85-59c5-4ef6-b6aa-e771a0ca6445",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Similarly to the Naive Bayes Classifier, Logistic Regression also does not offer hyperparameters that are very fundamental to the algorithm. If you check the documentation of [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), already performs L2 regularization by default. An important parameter is the `solver`. Note that the class constructor does not provide any parameter for a learning rate. This is because Logistic Regression can be solved much faster than using basic Gradient Descent. This is mainly due to loss function being convex, so more optimized techniques can be applied. The reason we cover Logistic Regression with Gradient Descent in the lecture is because it is a more general approach that is directly applicable to neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393bb0a-759c-4616-9bcf-07695d21f798",
   "metadata": {},
   "source": [
    "### Selecting the Best Maximum N-Gram Size\n",
    "\n",
    "Like in the notebook for the Naive Bayes Classifier, let's evaluate the effect of different maximum n-gram sizes in the result (i.e., the f1 scores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037939d-a926-43de-8e01-9979dce54d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ngram_size = 1\n",
    "max_ngram_size = 5\n",
    "\n",
    "num_runs = max_ngram_size - min_ngram_size\n",
    "\n",
    "# numpy array to keep track of all results\n",
    "results = []\n",
    "\n",
    "with tqdm(total=num_runs) as pbar:\n",
    "    for i, ngram in enumerate(range(min_ngram_size, max_ngram_size+1)):\n",
    "        # Create Document-Term Matrix for different n-gram sizes\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngram), max_features=20000)\n",
    "        X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "        X_test = tfidf_vectorizer.transform(sentences_test)\n",
    "        # Train & test model using cross validation\n",
    "        model = LogisticRegression()\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=10, scoring=\"f1\")\n",
    "        mean_score = np.mean(scores)\n",
    "        results.append((ngram, mean_score))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3272a6-2ba1-48c6-b9dd-8e67439194a5",
   "metadata": {},
   "source": [
    "With the f1 scores for the different values for `max_ngram_size`, we can quickly plot those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074f52-575e-4b32-b34f-d8f894692be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([s[0] for s in results], [s[1] for s in results], lw=3)\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Max N-Gram Size\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ace1a-ac44-4769-a922-0ccbdecfd2e4",
   "metadata": {},
   "source": [
    "While the best value for the maximum n-gram size is at 2, keep in mind that the f1 score actually doesn't change too much; see the scale of the y-axis. The main reason for this is that, for example, a maximum n-gram size of 3 still contains all unigrams and bigrams. This is the most common approach in practice. However, feel free to also set `min_ngram_size` to a larger value than 1 and see how it affects the results.\n",
    "\n",
    "Of course, all these results and observations only hold true for this specific data set and might significantly differ for other ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d086fbb-fb04-4ea0-92d2-263ad01cdbbe",
   "metadata": {},
   "source": [
    "## Pipelines & Grid Search\n",
    "\n",
    "Hyperparameter tuning is a quite important step, but the previous example has shown that it can be quite tedious. However, note that we basically tried all possible combinations for certain sets of parameter values. And since we were tuning 2 parameters, we required 2 nested loops. Thus, if we would tune $N$ parameters at the same time, we would need to have $N$ nested loops. Luckily, scikit-learn makes this much easier using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Since the parameters we would like to tune refer to 2 different components -- the vectorizer and the classifier -- we also need a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to combine both components into a single model. Let's do this first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c7b3e-1420-47c1-9754-2758b5d5684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a091c-297e-4b3c-aa6e-74cdb624eeae",
   "metadata": {},
   "source": [
    "Now we can define the search space, by providing the set of values for the hyperparameters we want to consider. See how the identifier of the parameters are a combination of the name in the pipeline (here: `tfidf` and `logreg`) and the name of the parameter in the respective class. For example, `tfidf__max_df` refers to the `max_df` parameter of the `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e536653-6517-441d-be24-3c04b0324547",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_features': (5000, 10000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'logreg__penalty': (\"l2\", \"elaticnet\"),\n",
    "    'logreg__fit_intercept': (True, False),\n",
    "    'logreg__C': (1.0, 10.0),\n",
    "    'logreg__solver': ('sag',)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc595c-5ddb-4f59-8d8a-9814d9908792",
   "metadata": {},
   "source": [
    "Now we can use `GridSearchCV` to check all possible combinations of parameter values. Of course, we kept the number of possible values rather small to avoid overly run times here. Note that for any parameter not listed above (e.g., `min_df` of the vectorizer) the default value is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7fcb4-df11-49e9-970e-27a027235968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1, cv=5)\n",
    "\n",
    "grid_search = grid_search.fit(sentences_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698655b5-2a5c-4bea-9344-6e3cb4f2e97a",
   "metadata": {},
   "source": [
    "Once the `GridSearchCV` has checked all possible parameter combinations, we can read out the best combination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7777bc-075f-4431-930b-031299a7f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e63200-c031-44e0-9778-b9fe3070a1cf",
   "metadata": {},
   "source": [
    "With these best parameter values -- note that those might not really be the best values as we selected just some alternatives for this example -- we compute the final scores by vectorizing our data and training the Naive Bayes Classifiers using those parameters. Now we train the classifier using the complete training data, and evaluate the classifier over the test data. Appreciate that we used the test data only this one time for the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6797bf5-5e9d-44cf-ad38-304de3ca2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000, max_df=0.75)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "X_test = tfidf_vectorizer.transform(sentences_test)\n",
    "\n",
    "model = LogisticRegression(C=10.0, fit_intercept=False).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precison: {:.3f}\".format(precision))\n",
    "print(\"Recall:   {:.3f}\".format(recall))\n",
    "print(\"F1 score: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc05bf6-8cd9-4c25-963d-3dd4ef68ee28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28c403-e8ce-4c3b-a8b4-370be62803b5",
   "metadata": {},
   "source": [
    "## Logistic Regression with PyTorch\n",
    "\n",
    "We have seen in the lecture that Logistic Regression forms the building block of neural networks: the neuron. This means we can implement Logistic Regression as a single-neuron network using PyTorch. While this is not meaningful in practice -- since Logistic Regression can be solved much more efficiently than with Gradient Descent / Backpropagation -- we can use this idea for a basic introduction into PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46929e0-b1db-474c-a7b8-59992331631f",
   "metadata": {},
   "source": [
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2360b-3dcf-4eda-8208-1a1d68536e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c143f0-1e8e-4397-8463-47f36fc6e4c5",
   "metadata": {},
   "source": [
    "### Checking/Setting the Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPU to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is small and our network model is very simple. In fact, the training is fast on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403481e-9c29-4c75-921a-6bcf898a6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec95943-dfc7-4e45-ad6a-b5e3bb4f85eb",
   "metadata": {},
   "source": [
    "### Create Tensors\n",
    "\n",
    "Both `X_train` and `X_test` are now our matrices containing the document vectors of our training and test set. However, right now, `X_train` and `X_test` are sparse matrices, i.e., representations that only store the non-zero values. Further use with PyTorch, we have to perform 2 additional steps:\n",
    "\n",
    "* Convert the sparse representation to a dense (i.e., full/normal) representation using `.todense()`; the output will be numpy arrays\n",
    "\n",
    "* Convert numpy arrays to tensors. `Tensor` is the data object used by PyTorch; they look, feel, and handle basically the same as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6601d-bd97-4467-8d7c-ad73f2d6c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "X_test = tfidf_vectorizer.transform(sentences_test)\n",
    "\n",
    "X_train = torch.Tensor(X_train.todense())\n",
    "X_test = torch.Tensor(X_test.todense())\n",
    "\n",
    "y_train = torch.Tensor(y_train).float()\n",
    "y_test = torch.Tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88d36e-03f4-4927-8479-4b21663e7ce9",
   "metadata": {},
   "source": [
    "### Create PyTorch Datasets\n",
    "\n",
    "Training a neural network is usually not done computing the gradient w.r.t. the whole dataset as most of the time the dataset is way too large to fit into memory. It might also slow down training since the gradients w.r.t. the whole dataset can be very small (although this could be addressed by increasing the learning rate). In practice, the training is basically always done using batches, i.e., much smaller subset of the data. While we can take `X_train` and `X_test` and implement our own loops to create batches, PyTorch comes with a series of convenient utility classes to simplify things.\n",
    "\n",
    "The first utility class we use is the [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class; more specifically, since `Dataset` is in abstract class, we use the [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class to wrap our tensors and make each sample retrievable by indexing the tensors along the first dimension. This will be used by the data loaders below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66490c64-0c30-4ae3-ab19-843a8059baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92e289-81ef-40ad-a1fc-60aa42df7ea7",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `DataSet` object as input to handle to split the dataset into batches. As such, a data loader also has `batch_size` as an input parameter. In the following, we use a batch size of 64, although you can easily go higher since we are dealing with only sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc668f-28d5-4d7c-8d07-88583f9866f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11140cca-5974-4cac-a6e6-1a41ed6efed8",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "The code cell below implements Logistic Regression as a neural network in PyTorch. Recall that we first compute the linear signal, which is the weighted sum of the input values. This is accomplished by the [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer. The main parameters of this class are the input size (i.e., the number of features / vocabulary size) and the output size which is just 1 for Logistic Regression. The internal parameters of `nn.Linear` are our weights $\\theta$ which also includes the bias $\\theta_0$. Since `nn.Linear` only gives us our linear signal, we need to push it through the Sigmoid function to get proper probabilities. PyTorch offers the `nn.Sigmoid` layer for this (PyTorch calls activations functions also layers).\n",
    "\n",
    "So far, we only defined our layers. The `forward()` method is the default method that is executed when a class instance evaluates an input. In this method, simply speaking, the input is pushed through all the layers. The `squeeze()` method is needed here to reshape `out` from the shape `(num_inputs, 1)` to just `(num_inputs,)`. Ensuring the proper shapes of all the tensors to avoid errors is just something to get used to when working with PyTorch or other frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9896599-2cac-445f-a407-77ef48702d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # Define fully connected (i.e., linear) layer\n",
    "        self.fc = nn.Linear(self.vocab_size, 1)\n",
    "        # Define sigmoid layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.fc(X)\n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ede176-273f-4d08-ac16-37d1e66d9283",
   "metadata": {},
   "source": [
    "To \"visualize\" the network, we can create and print an instance of the class `MyLogisticRegression`. Of course, due to its simplicity -- we only have 1 neuron after all -- the output does not look very exciting. The command `.to(device)` \"moves\" the instance to the selected instance (e.g., the CPU or GPU). In general, both the model and the data need to reside on the same instance. So if the model will be on the GPU, we also need to move the data later to the GPU. Using consistently `.to(device)` on the model and the data (see below) ensures that there will be no mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c479be4-8253-4e39-ab04-cd4bad733583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "classifier = MyLogisticRegression(X_train.shape[1]).to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931928b-50f8-43c8-b664-f6941e6a11a4",
   "metadata": {},
   "source": [
    "### Train & Evaluate Model\n",
    "\n",
    "With the data prepared and the model architecture defined, we can now train and evaluate a model to build our sentiment classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fcca0f-1660-403e-81d2-b62ed60a92d7",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. task with more than 2 classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89c567-bba3-4bcf-8a04-22000dd31cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "\n",
    "    # Set model to \"eval\" mode (not needed here, but a good practice)\n",
    "    model.eval()\n",
    "\n",
    "    # Collect predictions and ground truth for all samples across all batches\n",
    "    y_pred, y_test = [], []\n",
    "\n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "        \n",
    "        # Loop over each batch in the data loader\n",
    "        for X_batch, y_batch in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Push batch through network to get the log probability for each sample in the batch\n",
    "            probs = model(X_batch)                \n",
    "            \n",
    "            y_batch_pred = (probs>0.5).float()\n",
    "\n",
    "            # Add predictions and ground truth for current batch\n",
    "            y_test += list(y_batch.detach().cpu())\n",
    "            y_pred += list(y_batch_pred.detach().cpu())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()            \n",
    "            \n",
    "    # Return the f1 score as the output result\n",
    "    return metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fc6fe-bb7f-4f29-9aa5-6cbf4cf1c772",
   "metadata": {},
   "source": [
    "For a quick test, let's evaluate the newly created model. Of course, we didn't train our model, but it will still make predictions based on the initial weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b86018-0caa-4eee-925d-e67c2cead10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(classifier, loader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cf29e-198f-4136-975c-6454575a1efc",
   "metadata": {},
   "source": [
    "### Train Model (and evaluate after each epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train()` to wrap all the required steps training. This has the advantage that we can simply call `train()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `loader_train` and `loader_test`: this allows us to compute the f1 score over the training data an the test data after each epoch; we can later visualize the changes in the f1 scores\n",
    "\n",
    "* `optimizer`: the optimizer specifier how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "* `num_epochs`: the number of epochs -- i.e., the number of times we want train over all samples in our dataset\n",
    "\n",
    "The heart of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch if have to set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculating all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63a2d1-ddca-453f-9290-f6fc0d93233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs):\n",
    "\n",
    "    losses, f1_train, f1_test = [], [], []\n",
    "    \n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()\n",
    "\n",
    "    # Run all epochs\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(total=len(loader_train)) as pbar:\n",
    "\n",
    "            for X_batch, y_batch in loader_train:\n",
    "\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                probs = classifier(X_batch)                \n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(probs, y_batch)\n",
    "\n",
    "                ### Pytorch magic! ###\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        f1_tr = evaluate(model, loader_train)\n",
    "        f1_te = evaluate(model, loader_test)\n",
    "        \n",
    "        f1_train.append(f1_tr)\n",
    "        f1_test.append(f1_te)\n",
    "\n",
    "        print(\"Loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} (epoch {})\".format(epoch_loss, f1_tr, f1_te, epoch))\n",
    "        \n",
    "    return losses, f1_train, f1_test        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123f6f0-675f-4fc5-ba67-802b4f322d32",
   "metadata": {},
   "source": [
    "Before we can actually train the model, we need to instantiate the `criterion` (i.e., the loss function) and the `optimizer`. Conveniently, PyTorch provides the Binary Cross Entropy as a layer [`nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html). For the optimizer, we pick the widely used [`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer. Adam optimizer is the extended version of stochastic gradient descent which could be implemented in various deep learning applications such as computer vision and natural language processing in the future years. Adam was first introduced in 2014. The name is derived from adaptive moment estimation. The optimizer is called Adam because it uses estimations of the first and second moments of the gradient to adapt the learning rate for each weight of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f15d30-0bfa-41c8-a204-71e01fe498eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01)\n",
    "# Define loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4b137-f828-48fd-89e7-4ace63a714cc",
   "metadata": {},
   "source": [
    "Now we finally have everything in place to train the model. For this, we now only need to call the `train()` in the code cell below. Note that you can run the code cell below multiple times to continue the training for further 10 epochs. Each epoch will print 3 progress bars:\n",
    "\n",
    "* training over training set\n",
    "\n",
    "* evaluating over training set\n",
    "\n",
    "* evaluating over test set\n",
    "\n",
    "After each epoch, a print statement will show the current loss as well as the latest f1 scores for the training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a3b09-4bcf-4006-bb8c-78d843682706",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "losses, f1_train, f1_test = train(classifier, loader_train, loader_test, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc491ee-25cd-424e-9a33-e48c250ad3f3",
   "metadata": {},
   "source": [
    "Since the method `train()` returns the losses and f1 scores for each epoch, we can use this data to visualize how the loss and the f1 scores change over time, i.e., after each epoch. The code cell below creates the corresponding plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40086674-fe58-4708-967b-b337e3a72df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(losses)+1))\n",
    "\n",
    "# Convert losses to numpy array\n",
    "losses = np.asarray(losses)\n",
    "# Normalize losses so they match the scale in the plot (we are only interested in the trend of the losses!)\n",
    "losses = losses/np.max(losses)\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, losses, lw=3)\n",
    "plt.plot(x, f1_train, lw=3)\n",
    "plt.plot(x, f1_test, lw=3)\n",
    "\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "\n",
    "plt.gca().set_xticks(x)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.legend(['Loss', 'F1 (train)', 'F1 (test)'], loc='lower left', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d997aed-56d4-47a0-868f-6c95491e32ff",
   "metadata": {},
   "source": [
    "From the plot, we can observe several things\n",
    "\n",
    "* The loss goes down! This essentially means that our model is learning. This can be very useful as an incorrectly implemented model may not throw an error but not train properly (i.e., the loss not going down). So it's a good sanity check when implementing and using a new model.\n",
    "\n",
    "* The f1 score over the training data is getting close to 1.0 (at least after more epochs). This means that the model learns to correctly predict the label for all training samples. Of course, this is not what we are interested in but (a) again tells us that the model is generally learning, and (b) this might give us insights if our model is overfitting.\n",
    "\n",
    "* The f1 score over the test data quickly plateaus after very few epochs.\n",
    "\n",
    "**Important:** This plot showing the trends for the loss and f1 scores (or other metrics) can look very different depending on the dataset, the network architecture, and the hyperparameters. While in this simple example all trends seem to smoothly converge, this is not the case in general. For example, the f1 score for the test data might see a much steeper drop after peaking, which would even more clearly indicate the model is starting to overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380cbc6-e57a-4ad6-b077-825a2ddd6bfd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8362a1-cb79-4fc7-8009-fa92e4f92a22",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Logistic Regression is a foundational algorithm in machine learning, primarily used for binary classification tasks, including text document classification. In the context of text analysis, Logistic Regression operates by mapping text features—such as word frequencies, TF-IDF values, or word embeddings—into a probability score representing the likelihood of a document belonging to a specific class.\n",
    "\n",
    "For text document classification, each document undergoes a transformation into a numerical feature representation. This transformation captures the essence of the text, enabling Logistic Regression to learn a linear decision boundary that separates different classes based on these features. During training, Logistic Regression learns the weights (coefficients) for each feature and a bias term. It fits a linear model to distinguish between classes, utilizing the sigmoid function to produce probability scores. These probabilities are then thresholded to assign documents to their respective classes.\n",
    "\n",
    "Logistic Regression's strengths lie in its simplicity, interpretability, and computational efficiency. It serves as a baseline model for text classification tasks, especially when the relationships between features and classes are relatively linear or when there's a need for probabilistic interpretations of classification results. Overall, Logistic Regression provides a straightforward and effective means of predicting class probabilities for text documents based on their textual features, making it a valuable tool in tasks such as sentiment analysis, document categorization, and spam filtering in natural language processing applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded020b-9031-4717-9fc0-cd96bbdbb432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
